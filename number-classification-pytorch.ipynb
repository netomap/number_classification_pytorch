{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.dataloader import DataLoader\n\nfrom random import choice, random, randint\n\nfrom torchvision import transforms\nfrom torchvision import models as models\nfrom PIL import Image\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nfrom time import time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-08T15:10:01.620623Z","iopub.execute_input":"2021-09-08T15:10:01.621061Z","iopub.status.idle":"2021-09-08T15:10:01.627386Z","shell.execute_reply.started":"2021-09-08T15:10:01.621022Z","shell.execute_reply":"2021-09-08T15:10:01.626148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/digit-recognizer/train.csv')\ndf_train.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T15:10:01.629235Z","iopub.execute_input":"2021-09-08T15:10:01.629652Z","iopub.status.idle":"2021-09-08T15:10:03.953592Z","shell.execute_reply.started":"2021-09-08T15:10:01.629619Z","shell.execute_reply":"2021-09-08T15:10:03.952649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating a custom Dataset class","metadata":{}},{"cell_type":"code","source":"class customdataset(Dataset):\n\n    def __init__(self, lista, tipo):\n        self.lista = lista\n        self.tipo = tipo\n    \n    def __len__(self):\n        return len(self.lista)\n    \n    def __getitem__(self, index):\n        elemento = self.lista[index]\n        label = elemento[0]\n        img_numpy = elemento[1:].reshape(28,28)\n        return img_numpy, label, self.tipo\n\ndatasetteste = customdataset(df_train.values, 'train')\n\nplt.figure(figsize=(15, 4))\nfor k in range(5):\n    plt.subplot(1, 5, k+1)\n    img_numpy, label, tipo = choice(datasetteste)\n    plt.imshow(img_numpy)\n    plt.title(label)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T15:10:03.955354Z","iopub.execute_input":"2021-09-08T15:10:03.955692Z","iopub.status.idle":"2021-09-08T15:10:04.380755Z","shell.execute_reply.started":"2021-09-08T15:10:03.955655Z","shell.execute_reply":"2021-09-08T15:10:04.379812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now let's create a custom DataLoader","metadata":{}},{"cell_type":"code","source":"# Simple transformers\ntransformers = {\n    'train': transforms.Compose([\n        transforms.ToTensor(),\n        transforms.RandomRotation(16)\n    ]),\n    'test': transforms.Compose([\n        transforms.ToTensor()\n    ])\n}\n\n# First let's create a collate function that transform\ndef custom_collate_fn(batch):\n    imgs_batch, labels_batch = [], []\n    for img_numpy, label, tipo in batch:\n        imgs_batch.append(transformers[tipo](img_numpy))\n        labels_batch.append(label)\n    \n    labels_batch = torch.tensor(labels_batch)\n    imgs_batch = torch.stack(imgs_batch).type(torch.float)\n\n    return imgs_batch, labels_batch\n\n# creating simple df_train to validate a custom_collate_function\nsimple_dataset = customdataset(df_train.sample(8).values, 'train')\ndataloader = DataLoader(simple_dataset, batch_size=2, shuffle=True, collate_fn=custom_collate_fn)\nimgs_batch, labels_batch = next(iter(dataloader))\nprint (imgs_batch.shape)\nprint (labels_batch)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T15:10:04.38248Z","iopub.execute_input":"2021-09-08T15:10:04.382843Z","iopub.status.idle":"2021-09-08T15:10:04.395661Z","shell.execute_reply.started":"2021-09-08T15:10:04.382804Z","shell.execute_reply":"2021-09-08T15:10:04.394548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's create the model and yours criterion and optimizer","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint (f'device: {device}')\n\nmodel = torch.nn.Sequential(\n    torch.nn.Conv2d(in_channels=1, out_channels=128, kernel_size=3),\n    torch.nn.ReLU(),\n    torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3),\n    torch.nn.ReLU(),\n    torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3),\n    torch.nn.MaxPool2d(kernel_size=2),\n    torch.nn.ReLU(),\n    torch.nn.Dropout(0.5),\n    torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3),\n    torch.nn.ReLU(),\n    torch.nn.Dropout(0.5),\n    torch.nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3),\n    torch.nn.AdaptiveAvgPool2d(output_size=(10,10)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(in_features=64*10*10, out_features=1024),\n    torch.nn.Dropout(0.5),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=1024, out_features=1024),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=1024, out_features=10)\n)\n\nprint ('testing image passing throught model')\nimgs_batch, labels_batch = next(iter(dataloader))\n\noutput = model(imgs_batch)\nprint (f'output shape: {output.shape}')\n\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T15:10:04.397274Z","iopub.execute_input":"2021-09-08T15:10:04.397637Z","iopub.status.idle":"2021-09-08T15:10:04.488868Z","shell.execute_reply.started":"2021-09-08T15:10:04.3976Z","shell.execute_reply":"2021-09-08T15:10:04.487923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())","metadata":{"execution":{"iopub.status.busy":"2021-09-08T15:10:04.49016Z","iopub.execute_input":"2021-09-08T15:10:04.490515Z","iopub.status.idle":"2021-09-08T15:10:04.495425Z","shell.execute_reply.started":"2021-09-08T15:10:04.490478Z","shell.execute_reply":"2021-09-08T15:10:04.494241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now, creating a validation function that will be used at every end of epoch","metadata":{}},{"cell_type":"code","source":"probabilitys = torch.nn.Softmax(dim=1)\n\ndef validation_step(dataloader, debug=False):\n\n    begin_time = time()\n\n    if (debug):\n        print ('validation: ', end='')\n    \n    model.eval()\n    \n    dataloader_length = len(dataloader)\n    steps = 10\n    range = int(100/steps)\n    positions = steps*[False]\n\n    with torch.no_grad():\n        \n        validation_loss = 0\n        labels_pred, labels_true = [], []\n\n        for k, (imgs_batch, labels_batch) in enumerate(dataloader):\n\n            if (debug):\n                actual_position = int(100*k/dataloader_length) // range\n                if (not(positions[actual_position])):\n                    print (int(100*k/dataloader_length), end='%, ')\n                    positions[actual_position] = True\n\n            imgs_batch, labels_batch = imgs_batch.to(device), labels_batch.to(device)\n            output = model(imgs_batch)\n            loss = criterion(output, labels_batch)\n            validation_loss += loss.item()\n\n            labels_probabilitys = probabilitys(output)\n            labels_pred.append(labels_probabilitys.argmax(1))\n            labels_true.append(labels_batch)\n\n        labels_pred = torch.cat(labels_pred).to('cpu').numpy()\n        labels_true = torch.cat(labels_true).to('cpu').numpy()\n\n        accuracy = accuracy_score(labels_true, labels_pred)\n    \n    if (debug):\n        total_time = int(time() - begin_time)\n        print (f'100% in {total_time} sec')\n\n    return validation_loss, accuracy #, labels_pred, labels_true\n\nsimplecustomdataset = customdataset(df_train.sample(9).values, 'train')\nsimpledataloader = DataLoader(simplecustomdataset, batch_size=3, collate_fn=custom_collate_fn, shuffle=True)\n\nvalidation_step(simpledataloader)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T15:10:04.497138Z","iopub.execute_input":"2021-09-08T15:10:04.497561Z","iopub.status.idle":"2021-09-08T15:10:04.528947Z","shell.execute_reply.started":"2021-09-08T15:10:04.497522Z","shell.execute_reply":"2021-09-08T15:10:04.528023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating a one epoch function training","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(dataloader, debug=False):\n\n    begin_time = time()\n\n    if (debug):\n        print ('Training: ', end='')\n\n    model.train()\n\n    dataloader_length = len(dataloader)\n    steps = 10\n    range = int(100/steps)\n    positions = steps*[False]\n\n    epoch_loss = 0\n\n    for k, (imgs_batch, labels_batch) in enumerate(dataloader):\n        if (debug):\n            actual_position = int(100*k/dataloader_length) // range\n            if (not(positions[actual_position])):\n                print (int(100*k/dataloader_length), end='%, ')\n                positions[actual_position] = True\n    \n        imgs_batch, labels_batch = imgs_batch.to(device), labels_batch.to(device)\n\n        optimizer.zero_grad()\n        \n        output = model(imgs_batch)\n\n        loss = criterion(output, labels_batch)\n\n        epoch_loss += loss.item()\n\n        loss.backward()\n\n        optimizer.step()\n    \n    if (debug):\n        total_time = int(time() - begin_time)\n        print (f'100% in {total_time} sec')\n    \n    return epoch_loss\n\nsimplecustomdataset = customdataset(df_train.sample(1024).values, 'train')\nsimpledataloader = DataLoader(simplecustomdataset, batch_size=8, collate_fn=custom_collate_fn, shuffle=True)\n\ntrain_one_epoch(simpledataloader, debug=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T15:10:04.531306Z","iopub.execute_input":"2021-09-08T15:10:04.531642Z","iopub.status.idle":"2021-09-08T15:10:05.999022Z","shell.execute_reply.started":"2021-09-08T15:10:04.531607Z","shell.execute_reply":"2021-09-08T15:10:05.998017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's create train-validation dataset","metadata":{}},{"cell_type":"code","source":"df_train2, df_validation = train_test_split(df_train, test_size=0.07, shuffle=True)\nprint (f'len df_train2: {len(df_train2)}')\nprint (f'len df_validation: {len(df_validation)}')\n\ndataset_train = customdataset(df_train2.values, 'train')\ndataset_validation = customdataset(df_validation.values, 'test')\n\nBATCH_SIZE = 128\n\ntrain_dataloader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\nvalidation_dataloader = DataLoader(dataset_validation, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n\nprint (f'Batch_size: {BATCH_SIZE}')\nprint (f'len train_dataloader: {len(train_dataloader)}')\nprint (f'len validation_dataloader: {len(validation_dataloader)}')","metadata":{"execution":{"iopub.status.busy":"2021-09-08T15:10:06.00318Z","iopub.execute_input":"2021-09-08T15:10:06.00515Z","iopub.status.idle":"2021-09-08T15:10:06.256366Z","shell.execute_reply.started":"2021-09-08T15:10:06.005109Z","shell.execute_reply":"2021-09-08T15:10:06.255454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"code","source":"results = []\n\nfor epoch in range(6):\n    print (f'Epoch: {epoch}')\n    epoch_loss = train_one_epoch(train_dataloader, debug=True)\n    validation_loss, validation_accuracy = validation_step(validation_dataloader, debug=True)\n    print ('epoch_loss {:0.2f}, validation_loss: {:0.2f}, validation_accuracy: {:0.2f}'.format(epoch_loss, validation_loss, validation_accuracy))\n    print (50*'-')\n    results.append([epoch, epoch_loss, validation_loss, validation_accuracy])","metadata":{"execution":{"iopub.status.busy":"2021-09-08T15:10:06.257611Z","iopub.execute_input":"2021-09-08T15:10:06.25811Z","iopub.status.idle":"2021-09-08T15:11:42.235107Z","shell.execute_reply.started":"2021-09-08T15:10:06.258071Z","shell.execute_reply":"2021-09-08T15:11:42.234301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ploting some results","metadata":{}},{"cell_type":"code","source":"results = np.array(results)\nplt.figure(figsize=(18,4))\n\nplt.subplot(1, 3, 1)\nplt.plot(results[:,0], results[:,1])\nplt.title('Epoch Loss')\n\nplt.subplot(1, 3, 2)\nplt.plot(results[:,0], results[:,2])\nplt.title('Validation Loss')\n\nplt.subplot(1, 3, 3)\nplt.plot(results[:,0], results[:,3])\nplt.title('Validation Accuracy')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T15:11:42.236385Z","iopub.execute_input":"2021-09-08T15:11:42.236709Z","iopub.status.idle":"2021-09-08T15:11:42.592443Z","shell.execute_reply.started":"2021-09-08T15:11:42.236673Z","shell.execute_reply":"2021-09-08T15:11:42.591695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now, predicting test dataset","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv('../input/digit-recognizer/test.csv')\ndf_test.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T15:11:42.593667Z","iopub.execute_input":"2021-09-08T15:11:42.594005Z","iopub.status.idle":"2021-09-08T15:11:44.297694Z","shell.execute_reply.started":"2021-09-08T15:11:42.593967Z","shell.execute_reply":"2021-09-08T15:11:44.296899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class custom_test_dataset(Dataset):\n\n    def __init__(self, lista):\n        self.lista = lista\n    \n    def __len__(self):\n        return len(self.lista)\n    \n    def __getitem__(self, index):\n        elemento = self.lista[index]\n        img_numpy = elemento.reshape(28,28)\n        return img_numpy\n\ntest_dataset = custom_test_dataset(df_test.values)\nplt.imshow(choice(test_dataset))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T15:11:44.299015Z","iopub.execute_input":"2021-09-08T15:11:44.299363Z","iopub.status.idle":"2021-09-08T15:11:44.422954Z","shell.execute_reply.started":"2021-09-08T15:11:44.299326Z","shell.execute_reply":"2021-09-08T15:11:44.422169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def custom_collate_test_fn(batch):\n    imgs_batch = []\n    for img_numpy in batch:\n        imgs_batch.append(torch.tensor(img_numpy, dtype=torch.float).unsqueeze_(0))\n    \n    imgs_batch = torch.stack(imgs_batch)\n\n    return imgs_batch\n\nsimple_test_dataloader = DataLoader(test_dataset, batch_size=2, collate_fn=custom_collate_test_fn, shuffle=True)\nimgs_test_batch = next(iter(simple_test_dataloader))\nprint (imgs_test_batch.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T15:11:44.425634Z","iopub.execute_input":"2021-09-08T15:11:44.425903Z","iopub.status.idle":"2021-09-08T15:11:44.437277Z","shell.execute_reply.started":"2021-09-08T15:11:44.425876Z","shell.execute_reply":"2021-09-08T15:11:44.43646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting the test dataset","metadata":{}},{"cell_type":"code","source":"test_dataset = custom_test_dataset(df_test.values)\ntest_dataloader = DataLoader(test_dataset, batch_size=2, collate_fn=custom_collate_test_fn)\n\nmodel.eval()\n\nlabels_pred = []\n\nwith torch.no_grad():\n    for imgs_batch in test_dataloader:\n        imgs_batch = imgs_batch.to(device)\n        output = model(imgs_batch)\n        labels_probabilitys = probabilitys(output)\n        labels_pred.append(labels_probabilitys.argmax(1))\n\nlabels_pred = torch.cat(labels_pred).to('cpu').numpy()\n\nlabels_pred","metadata":{"execution":{"iopub.status.busy":"2021-09-08T15:11:44.43861Z","iopub.execute_input":"2021-09-08T15:11:44.439058Z","iopub.status.idle":"2021-09-08T15:12:11.379048Z","shell.execute_reply.started":"2021-09-08T15:11:44.439019Z","shell.execute_reply":"2021-09-08T15:12:11.378233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Verifying the 10 first results","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(18, 7))\n\nfor k in range(10):\n    plt.subplot(2, 5, k+1)\n    img_numpy = test_dataset[k].reshape(28,28)\n    label_pred = labels_pred[k]\n    plt.imshow(img_numpy)\n    plt.title(label_pred)\n    plt.xticks([])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T15:12:11.380376Z","iopub.execute_input":"2021-09-08T15:12:11.380704Z","iopub.status.idle":"2021-09-08T15:12:12.063607Z","shell.execute_reply.started":"2021-09-08T15:12:11.380669Z","shell.execute_reply":"2021-09-08T15:12:12.062773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating submission.csv file","metadata":{}},{"cell_type":"code","source":"submission_array = [[k+1, label] for k, label in enumerate(labels_pred)]\nsubmission_dataset = pd.DataFrame(submission_array, columns=['ImageId','Label'])\n\nsubmission_dataset.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T15:12:12.064855Z","iopub.execute_input":"2021-09-08T15:12:12.06522Z","iopub.status.idle":"2021-09-08T15:12:12.152522Z","shell.execute_reply.started":"2021-09-08T15:12:12.065181Z","shell.execute_reply":"2021-09-08T15:12:12.151736Z"},"trusted":true},"execution_count":null,"outputs":[]}]}